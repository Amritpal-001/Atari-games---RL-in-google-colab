{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:universe]",
      "language": "python",
      "name": "conda-env-universe-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "Openai Pacman- works.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evhp6qWY3Zwu",
        "colab_type": "text"
      },
      "source": [
        "# Building an Agent to Play Atari games using Deep Q Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0cPQwPhx3Zw0",
        "colab_type": "text"
      },
      "source": [
        "First we import all the necessary libraries </font> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1TiKpDp3ye4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.13.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiL5OS1D3Zw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "2471ae70-25a6-4bf5-89fc-078c349e6466"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "XBF57xmz3Zxi",
        "colab_type": "text"
      },
      "source": [
        "Now we define a function called preprocess_observation for preprocessing our input game screen. We reduce the image size\n",
        "and convert the image into greyscale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPzIQd_e3Zxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "color = np.array([210, 164, 74]).mean()\n",
        "\n",
        "def preprocess_observation(obs):\n",
        "\n",
        "    # Crop and resize the image\n",
        "    img = obs[1:176:2, ::2]\n",
        "\n",
        "    # Convert the image to greyscale\n",
        "    img = img.mean(axis=2)\n",
        "\n",
        "    # Improve image contrast\n",
        "    img[img==color] = 0\n",
        "\n",
        "    # Next we normalize the image from -1 to +1\n",
        "    img = (img - 128) / 128 - 1\n",
        "\n",
        "    return img.reshape(88,80,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Ia9uNO3Zx_",
        "colab_type": "text"
      },
      "source": [
        " Let us initialize our gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-uWcHnk3ZyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"MsPacman-v0\")\n",
        "n_outputs = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "noV2ZUk83Zya",
        "colab_type": "text"
      },
      "source": [
        "Okay, Now we define a function called q_network for building our Q network. We input the game state\n",
        "to the Q network and get the Q values for all the actions in that state. <br><br>\n",
        "We build Q network with three convolutional layers with same padding followed by a fully connected layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVT5J-D63Zyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "def q_network(X, name_scope):\n",
        "    \n",
        "    # Initialize layers\n",
        "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
        "\n",
        "    with tf.variable_scope(name_scope) as scope: \n",
        "\n",
        "        # initialize the convolutional layers\n",
        "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
        "        tf.summary.histogram('layer_1',layer_1)\n",
        "        \n",
        "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
        "        tf.summary.histogram('layer_2',layer_2)\n",
        "        \n",
        "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "        tf.summary.histogram('layer_3',layer_3)\n",
        "        \n",
        "        # Flatten the result of layer_3 before feeding to the fully connected layer\n",
        "        flat = flatten(layer_3)\n",
        "\n",
        "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
        "        tf.summary.histogram('fc',fc)\n",
        "        \n",
        "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
        "        tf.summary.histogram('output',output)\n",
        "        \n",
        "\n",
        "        # Vars will store the parameters of the network such as weights\n",
        "        vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
        "        return vars, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObcXSwrR3Zy0",
        "colab_type": "text"
      },
      "source": [
        "Next we define a function called epsilon_greedy for performing epsilon greedy policy. In epsilon greedy policy we either select the best action with probability 1 - epsilon or a random action with\n",
        "probability epsilon.\n",
        "\n",
        "We use decaying epsilon greedy policy where value of epsilon will be decaying over time as we don't want to explore\n",
        "forever. So over time our policy will be exploiting only good actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziFSt9ZY3Zy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.5\n",
        "eps_min = 0.05\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 500000\n",
        "\n",
        "def epsilon_greedy(action, step):\n",
        "    p = np.random.random(1).squeeze()\n",
        "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybnhUyNU3ZzQ",
        "colab_type": "text"
      },
      "source": [
        "Now, we initialize our experience replay buffer of length 20000 which holds the experience.\n",
        "\n",
        "We store all the agent's experience i.e (state, action, rewards) in the experience replay buffer\n",
        "and  we sample from this minibatch of experience for training the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMaRGuK93ZzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buffer_len = 20000\n",
        "exp_buffer = deque(maxlen=buffer_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJc9wlAh3Zzm",
        "colab_type": "text"
      },
      "source": [
        "Next, we define a function called sample_memories for sampling experiences from the memory. Batch size is the number of experience sampled\n",
        "from the memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziQdWGh83Zzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_memories(batch_size):\n",
        "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
        "    mem = np.array(exp_buffer)[perm_batch]\n",
        "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ZUqeT83Zz-",
        "colab_type": "text"
      },
      "source": [
        "Now we define our network hyperparameters,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV6x8viS3Z0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 800\n",
        "batch_size = 48\n",
        "input_shape = (None, 88, 80, 1)\n",
        "learning_rate = 0.001\n",
        "X_shape = (None, 88, 80, 1)\n",
        "discount_factor = 0.97\n",
        "\n",
        "global_step = 0\n",
        "copy_steps = 100\n",
        "steps_train = 4\n",
        "start_steps = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OFWuYE33Z0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = 'logs'\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Now we define the placeholder for our input i.e game state\n",
        "X = tf.placeholder(tf.float32, shape=X_shape)\n",
        "\n",
        "# we define a boolean called in_training_model to toggle the training\n",
        "in_training_mode = tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y2V_paN3Z0n",
        "colab_type": "text"
      },
      "source": [
        " Now let us build our primary and target Q network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZsVdytn3Z0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "474b791b-076a-4440-a254-b9eec657e07a"
      },
      "source": [
        "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
        "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
        "\n",
        "# similarly we build our target Q network\n",
        "targetQ, targetQ_outputs = q_network(X, 'targetQ')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtThXsGx3Z1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5504ba9d-14cc-4d98-8a11-44632c3f5d42"
      },
      "source": [
        "# define the placeholder for our action values\n",
        "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
        "Q_action = tf.reduce_sum(targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keep_dims=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-2e17235474d5>:3: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW1gPnT93Z1X",
        "colab_type": "text"
      },
      "source": [
        "Copy the primary Q network parameters to the target  Q network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJMPXdoU3Z1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
        "copy_target_to_main = tf.group(*copy_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dSiS6RW3Z1s",
        "colab_type": "text"
      },
      "source": [
        "Compute and optimize loss using gradient descent optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34JVJ8Mi3Z1y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "540688ed-3649-4cb3-8128-b6d5eb9fbdf8"
      },
      "source": [
        "# define a placeholder for our output i.e action\n",
        "y = tf.placeholder(tf.float32, shape=(None,1))\n",
        "\n",
        "# now we calculate the loss which is the difference between actual value and predicted value\n",
        "loss = tf.reduce_mean(tf.square(y - Q_action))\n",
        "\n",
        "# we use adam optimizer for minimizing the loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "loss_summary = tf.summary.scalar('LOSS', loss)\n",
        "merge_summary = tf.summary.merge_all()\n",
        "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFFRCuCw3Z2C",
        "colab_type": "text"
      },
      "source": [
        " Now we start the tensorflow session and run the model,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "E6Dc1g703Z2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "ac207fc0-c299-4b3a-fd40-592f89aa6e99"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    \n",
        "    # for each episode\n",
        "    for i in range(num_episodes):\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        epoch = 0\n",
        "        episodic_reward = 0\n",
        "        actions_counter = Counter() \n",
        "        episodic_loss = []\n",
        "\n",
        "        # while the state is not the terminal state\n",
        "        while not done:\n",
        "\n",
        "           #env.render()\n",
        "        \n",
        "            # get the preprocessed game screen\n",
        "            obs = preprocess_observation(obs)\n",
        "\n",
        "            # feed the game screen and get the Q values for each action\n",
        "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "\n",
        "            # get the action\n",
        "            action = np.argmax(actions, axis=-1)\n",
        "            actions_counter[str(action)] += 1 \n",
        "\n",
        "            # select the action using epsilon greedy policy\n",
        "            action = epsilon_greedy(action, global_step)\n",
        "            \n",
        "            # now perform the action and move to the next state, next_obs, receive reward\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store this transistion as an experience in the replay buffer\n",
        "            exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
        "            \n",
        "            # After certain steps, we train our Q network with samples from the experience replay buffer\n",
        "            if global_step % steps_train == 0 and global_step > start_steps:\n",
        "                \n",
        "                # sample experience\n",
        "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
        "\n",
        "                # states\n",
        "                o_obs = [x for x in o_obs]\n",
        "\n",
        "                # next states\n",
        "                o_next_obs = [x for x in o_next_obs]\n",
        "\n",
        "                # next actions\n",
        "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
        "\n",
        "\n",
        "                # reward\n",
        "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
        "\n",
        "                # merge all summaries and write to the file\n",
        "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
        "                file_writer.add_summary(mrg_summary, global_step)\n",
        "\n",
        "                # now we train the network and calculate loss\n",
        "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
        "                episodic_loss.append(train_loss)\n",
        "            \n",
        "            # after some interval we copy our main Q network weights to target Q network\n",
        "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
        "                copy_target_to_main.run()\n",
        "                \n",
        "            obs = next_obs\n",
        "            epoch += 1\n",
        "            global_step += 1\n",
        "            episodic_reward += reward\n",
        "        \n",
        "        print('Epoch', epoch, 'Reward', episodic_reward,)\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 644 Reward 220.0\n",
            "Epoch 654 Reward 220.0\n",
            "Epoch 827 Reward 190.0\n",
            "Epoch 598 Reward 170.0\n",
            "Epoch 629 Reward 300.0\n",
            "Epoch 576 Reward 230.0\n",
            "Epoch 659 Reward 260.0\n",
            "Epoch 656 Reward 150.0\n",
            "Epoch 679 Reward 280.0\n",
            "Epoch 617 Reward 200.0\n",
            "Epoch 716 Reward 280.0\n",
            "Epoch 672 Reward 230.0\n",
            "Epoch 627 Reward 340.0\n",
            "Epoch 628 Reward 260.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-62c9a637918a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m# merge all summaries and write to the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mmrg_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mo_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_action\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mo_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_training_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmrg_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5180\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdyHEQek6oqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGUmGUp24aq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}